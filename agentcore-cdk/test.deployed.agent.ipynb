{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dbb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from uuid import uuid4\n",
    "agent_core_client = boto3.client('bedrock-agentcore')\n",
    "agent_arn = 'YOUR_AGENT_ARN_FROM_OUTPUT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f100a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_agent_core(agent_arn, payload, session_id):\n",
    "    # Invoke the agent\n",
    "    response = agent_core_client.invoke_agent_runtime(\n",
    "        agentRuntimeArn=agent_arn,\n",
    "        runtimeSessionId=session_id,\n",
    "        payload=payload\n",
    "    )\n",
    "\n",
    "    # Process and print the response\n",
    "    if \"text/event-stream\" in response.get(\"contentType\", \"\"):\n",
    "    \n",
    "        for line in response[\"response\"].iter_lines(chunk_size=10):\n",
    "            if line:\n",
    "                line = line.decode(\"utf-8\")\n",
    "                if line.startswith(\"data: \"):\n",
    "                    line = line[6:]\n",
    "                    json_line = json.loads(line)\n",
    "\n",
    "                    if type(json_line) == dict:\n",
    "                        text = json_line.get(\"event\", {}).get(\"contentBlockDelta\", {}).get(\"delta\", {}).get(\"text\")    \n",
    "                        if text:\n",
    "                            print (text, end=\"\")\n",
    "\n",
    "\n",
    "    elif response.get(\"contentType\") == \"application/json\":\n",
    "        # Handle standard JSON response\n",
    "        content = []\n",
    "        for chunk in response.get(\"response\", []):\n",
    "            content.append(chunk.decode('utf-8'))\n",
    "        print(json.loads(''.join(content)))\n",
    "    \n",
    "    else:\n",
    "        # Print raw response for other content types\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you search for information about deploying generative AI agents to production. Let me create a research plan and then search for relevant resources.\n",
      "\n",
      "## Research Plan:\n",
      "1. Search AWS blogs for information on deploying Gen AI agents to production\n",
      "2. Look for best practices, tools, and architectural patterns\n",
      "3. Search for real-world examples and case studies\n",
      "4. Find technical guidance on deployment considerations\n",
      "\n",
      "Let me start by searching the AWS blogs:Great! I found several highly relevant articles. Let me extract the most relevant ones to provide you with comprehensive information:Let me try extracting each URL individually:Excellent! Now let me get more information on the remaining topics:Perfect! Now I have comprehensive information. Let me compile a detailed research summary for you:\n",
      "\n",
      "---\n",
      "\n",
      "# **Comprehensive Guide: Deploying Gen AI Agents to Production**\n",
      "\n",
      "## **Executive Summary**\n",
      "\n",
      "Deploying generative AI agents to production is more complex than traditional software deployments. Organizations face unique challenges including AI-specific failure modes, stochastic behavior, resource optimization, security isolation, and operational excellence requirements. AWS provides comprehensive services and frameworks to address these challenges, from Amazon Bedrock AgentCore Runtime to specialized observability and governance tools.\n",
      "\n",
      "---\n",
      "\n",
      "## **1. KEY CHALLENGES IN PRODUCTION DEPLOYMENT**\n",
      "\n",
      "Based on AWS's latest guidance, organizations struggle with **\"proof of concept purgatory\"** — where promising agent prototypes fail to reach production due to:\n",
      "\n",
      "### **Infrastructure & Scalability Challenges:**\n",
      "- **Variable resource needs**: AI agents consume unpredictable compute resources due to recursive processing and multi-step reasoning\n",
      "- **Capacity planning complexity**: Estimating Requests Per Minute (RPM) and Tokens Per Minute (TPM) is difficult due to the stochastic nature of agents\n",
      "- **Cold start penalties**: Traditional serverless solutions impose timeout limitations\n",
      "- **Session state management**: Maintaining context across multiple invocations without external persistence\n",
      "\n",
      "### **Security & Isolation Issues:**\n",
      "- **Multi-tenant data contamination**: As seen in the May 2025 Asana incident, cross-tenant data leakage can occur without proper session isolation\n",
      "- **Identity and access control**: Complex requirements for agents acting on behalf of users\n",
      "- **Credential management**: Agents need secure access to external services and internal systems\n",
      "\n",
      "### **Operational Complexity:**\n",
      "- **Framework fragmentation**: Teams forced to standardize on single frameworks, slowing innovation\n",
      "- **Latency management**: Balancing speed, cost, and accuracy\n",
      "- **Observability gaps**: Traditional monitoring insufficient for AI-specific signals\n",
      "- **Governance requirements**: Compliance, responsible AI, and model governance\n",
      "\n",
      "---\n",
      "\n",
      "## **2. SEVEN-DIMENSION RESILIENCE FRAMEWORK**\n",
      "\n",
      "AWS defines a comprehensive resilience risk analysis framework for AI agents:\n",
      "\n",
      "### **Dimension 1: Foundation Models (FMs)**\n",
      "- **Deployment choices**: Self-managed (EC2), managed services (SageMaker), or serverless (Amazon Bedrock)\n",
      "- **Resilience strategy**: Implement cross-Region inference with automated failover\n",
      "- **Cost implications**: Serverless models reduce upfront infrastructure costs\n",
      "\n",
      "### **Dimension 2: Agent Orchestration**\n",
      "- **Components**: Tool selection logic, human escalation triggers, multi-step workflow management\n",
      "- **Resilience patterns**: Circuit breakers, bounded retry limits with exponential backoff\n",
      "- **Contract-compatible fallbacks**: Route from failed tools to alternatives with consistent schemas\n",
      "\n",
      "### **Dimension 3: Agent Deployment Infrastructure**\n",
      "- **Options**: Self-managed EC2, managed services (Amazon ECS), specialized services (Amazon Bedrock AgentCore Runtime)\n",
      "- **Key consideration**: Amazon Bedrock AgentCore Runtime provides persistent microVM sessions (up to 8 hours) with complete isolation\n",
      "\n",
      "### **Dimension 4: Knowledge Base**\n",
      "- **Components**: Vector database, embedding models, data pipelines\n",
      "- **Production considerations**: Advanced parsing (handle tables/unstructured data), hierarchical/semantic chunking, metadata filtering\n",
      "\n",
      "### **Dimension 5: Agent Tools**\n",
      "- **Types**: API tools, Model Context Protocol (MCP) servers, memory management\n",
      "- **Scaling strategy**: Isolated Lambda functions with reserved concurrency per tool\n",
      "\n",
      "### **Dimension 6: Security & Compliance**\n",
      "- **Inbound authentication**: User access to agents\n",
      "- **Outbound authentication**: Agents accessing external systems\n",
      "- **Content compliance**: Automated reasoning checks, hallucination prevention\n",
      "- **Solution**: Amazon Bedrock AgentCore Identity provides embedded identity management\n",
      "\n",
      "### **Dimension 7: Evaluation & Observability**\n",
      "- **Traditional metrics**: Infrastructure statistics via CloudWatch\n",
      "- **AI-specific signals**: Reasoning traces, tool invocation results\n",
      "- **Solution**: Amazon Bedrock AgentCore Observability (preview) with OpenTelemetry support\n",
      "\n",
      "---\n",
      "\n",
      "## **3. FIVE CRITICAL FAILURE MODES & MITIGATION STRATEGIES**\n",
      "\n",
      "### **Failure Mode 1: Shared Fate**\n",
      "**Problem**: Failure in one component cascades across system boundaries\n",
      "\n",
      "**Mitigation Strategies**:\n",
      "- **Fault isolation**: Design tools as independent containment domains with versioned schemas\n",
      "- **Semantic validations**: Date ranges, cross-field rules, data freshness checks\n",
      "- **Circuit breakers**: Monitor failure rates and latency\n",
      "- **Graceful degradation**: Maintain core functionality while advanced features become unavailable\n",
      "\n",
      "### **Failure Mode 2: Insufficient Capacity**\n",
      "**Problem**: Excessive load overwhelms provisioned systems\n",
      "\n",
      "**Critical Equations**:\n",
      "```\n",
      "RPM = Average threads/minute × FM invocations/minute per thread\n",
      "    = Avg threads/minute × (1 + 60/(max_completion_tokens/TPS))\n",
      "\n",
      "TPM = RPM × Average input token length\n",
      "    = RPM × (system prompt + user prompt + max_completion_tokens × (recursion_limit-1)/recursion_limit)\n",
      "```\n",
      "\n",
      "**Mitigation Strategies**:\n",
      "- **Recursion limits**: Prevent runaway token consumption\n",
      "- **Max completion tokens**: Control growth in recursive reasoning chains\n",
      "- **Intelligent queuing**: Priority-based allocation during high load\n",
      "- **Reserved concurrency**: Allocate dedicated capacity per tool/action group\n",
      "\n",
      "### **Failure Mode 3: Excessive Latency**\n",
      "**Problem**: User experience degradation, reduced throughput\n",
      "\n",
      "**Optimization Techniques**:\n",
      "- **Prompt engineering**: Reduce unnecessary reasoning loops\n",
      "  - *Bad*: \"approve if strategically valuable\" (requires agent to define criteria)\n",
      "  - *Good*: Explicitly state all criteria in system prompt\n",
      "- **Prompt caching**: Store repeated prefixes, reducing latency up to 85%\n",
      "- **Asynchronous processing**: Enable parallel tool execution\n",
      "- **Streaming compliance scanning**: Real-time content moderation during generation\n",
      "\n",
      "### **Failure Mode 4: Incorrect Agent Response**\n",
      "**Problem**: Misconfiguration, software bugs, model hallucinations\n",
      "\n",
      "**Mitigation Strategies**:\n",
      "- **Deterministic orchestration**: Define explicit workflows rather than LLM improvisation\n",
      "- **Input/output guardrails**: Amazon Bedrock Guardrails scan for compliance, hallucinations\n",
      "- **Human-in-the-loop validation**: High-stakes decisions require human review\n",
      "- **Automatic retry with refined prompts**: Re-attempt with better instructions\n",
      "\n",
      "### **Failure Mode 5: Single Point of Failure**\n",
      "**Problem**: One component failure causes system-wide failure\n",
      "\n",
      "**Mitigation Strategies**:\n",
      "- **Multi-Region model deployment**: Automated failover for self-managed models\n",
      "- **Cross-Region inference**: Built-in Amazon Bedrock redundancy\n",
      "- **Tool redundancy**: Automatically route to alternative tools during failures\n",
      "- **Permission consistency**: Maintain identical access controls across redundant environments\n",
      "\n",
      "---\n",
      "\n",
      "## **4. AMAZON BEDROCK AGENTCORE RUNTIME: PRODUCTION-READY DEPLOYMENT**\n",
      "\n",
      "AWS's newest solution specifically addresses \"proof of concept purgatory\" with **Amazon Bedrock AgentCore Runtime**:\n",
      "\n",
      "### **Four Lines of Code to Production:**\n",
      "```python\n",
      "from bedrock_agentcore.runtime import BedrockAgentCoreApp\n",
      "\n",
      "app = BedrockAgentCoreApp()\n",
      "\n",
      "@app.entrypoint\n",
      "def my_agent(payload, context):\n",
      "    # Your agent logic here\n",
      "    return response\n",
      "\n",
      "app.run()\n",
      "```\n",
      "\n",
      "### **Key Capabilities:**\n",
      "\n",
      "#### **1. Framework & Model Agnostic**\n",
      "- Support for LangGraph, CrewAI, Strands, and custom frameworks\n",
      "- Model flexibility: Amazon Bedrock models, Anthropic Claude, OpenAI, Google Gemini\n",
      "- Zero migration required for existing codebases\n",
      "\n",
      "#### **2. Session Management & Persistent Execution**\n",
      "- **Session lifecycle**: Active (processing) → Idle (ready) → Terminated (>15 min inactivity or 8 hours max)\n",
      "- **Persistent state**: Maintain context across multiple invocations without external storage\n",
      "- **Large payloads**: Support up to 100 MB requests/responses\n",
      "- **Asynchronous tasks**: Multi-hour agent execution with `add_async_task()` and `complete_async_task()`\n",
      "\n",
      "#### **3. Security Isolation**\n",
      "- **True session isolation**: Dedicated microVMs with isolated compute, memory, file system\n",
      "- **Complete data separation**: Session termination triggers memory sanitization\n",
      "- **Protection against cross-contamination**: Prevents Asana-style data leakage incidents\n",
      "\n",
      "#### **4. Embedded Identity Management**\n",
      "- **Inbound authentication**: \n",
      "  - IAM SigV4 for AWS resources\n",
      "  - JWT Bearer tokens for enterprise identity providers (Cognito, Okta, Entra ID)\n",
      "- **Outbound authentication**: \n",
      "  - Cryptographic binding of user tokens to agent workload identity\n",
      "  - Prevents credential leakage across sessions\n",
      "\n",
      "#### **5. Consumption-Based Pricing**\n",
      "- **Pay only for active compute**: No charges during I/O waits (LLM calls, API invocations)\n",
      "- **Real-world impact**: 70% CPU cost reduction vs. traditional models\n",
      "  - Example: 60-second session with 18 seconds active processing = pay for 18 seconds, not 60\n",
      "- **Memory efficiency**: Charged only for actual consumption moment-by-moment\n",
      "\n",
      "### **Integration with State Persistence**\n",
      "\n",
      "**AgentCore Memory** provides:\n",
      "- **Short-term memory**: Raw interaction events via `create_event()`, retrieve with `get_last_k_turns()`\n",
      "- **Long-term memory**: Consolidated insights (preferences, facts, summaries) accessible across sessions via `retrieve_memories()`\n",
      "- **Automatic extraction**: LLM-driven consolidation without impacting real-time performance\n",
      "\n",
      "---\n",
      "\n",
      "## **5. MATURE GENERATIVE AI FOUNDATION ARCHITECTURE**\n",
      "\n",
      "Organizations should establish a comprehensive foundation with multiple components:\n",
      "\n",
      "### **Core Components:**\n",
      "\n",
      "**1. Hub Architecture**\n",
      "- **Model hub**: Central access to approved foundation models\n",
      "- **Tool/Agent hub**: Discovery and connectivity via MCP and Agent2Agent (A2A) protocols\n",
      "\n",
      "**2. Gateway Services**\n",
      "- **Multi-tenant access control**: Fine-grained authorization\n",
      "- **Unified APIs**: Model access, guardrails, evaluation\n",
      "- **Rate limiting & cost attribution**: Prevent abuse, track usage\n",
      "- **Prompt caching**: Reduce inference costs and latency\n",
      "- **Load balancing**: Distribute across regions and instances\n",
      "\n",
      "**3. Orchestration Layer**\n",
      "- **Deterministic flows**: RAG patterns with retrieval-augmented prompts\n",
      "- **Agent workflows**: LLM-driven reasoning and tool selection\n",
      "- **Multi-agent systems**: Specialized subagents for complex tasks\n",
      "\n",
      "**4. Model Customization**\n",
      "- **Continued pre-training**: Domain-adaptive fine-tuning\n",
      "- **Supervised fine-tuning**: Task-specific adaptation\n",
      "- **Alignment techniques**: RLHF, Direct Preference Optimization (DPO)\n",
      "\n",
      "**5. Data Management**\n",
      "- **Enterprise source integration**: Data lakes, warehouses, external sources\n",
      "- **RAG templates**: Pre-built blueprints for vector databases, chunking, embeddings\n",
      "- **Data processing pipelines**: Labeled and synthetic data generation\n",
      "- **Data cataloging**: Quick discovery and governance\n",
      "\n",
      "**6. GenAIOps (Generative AI Operations)**\n",
      "- **RAGOps**: Vector database optimization, indexing pipelines, retrieval strategies\n",
      "- **AgentOps**: Tool management, state machines, memory management\n",
      "- **ModelOps/FMOps/LLMOps**: Model lifecycle, experiments, registry, deployment\n",
      "- **CI/CD automation**: Integrate evaluation, prompt management, deployments\n",
      "\n",
      "**7. Observability & Responsible AI**\n",
      "- **Combined metrics**: Traditional (CloudWatch) + AI-specific (traces, reasoning)\n",
      "- **Responsible AI dimensions**: Privacy, safety, transparency, explainability, fairness\n",
      "- **Governance**: Model performance monitoring, data access controls\n",
      "- **Guardrails**: Input/output filtering, compliance checks\n",
      "\n",
      "### **Operating Models:**\n",
      "\n",
      "**Centralized**: Single platform team manages foundation for all LOBs\n",
      "**Decentralized**: LOBs build independently; central team acts as Center of Excellence\n",
      "**Federated (Recommended)**: Central foundation provides building blocks; LOBs build custom components\n",
      "\n",
      "### **Multi-Tenant Strategies:**\n",
      "\n",
      "- **Physical isolation**: Separate instances per tenant (strict isolation)\n",
      "- **Logical isolation**: Shared infrastructure with tenant identifiers\n",
      "- **Hybrid**: Core foundation shared; specific components isolated per tenant\n",
      "- **Noisy neighbor prevention**: Resource allocation, containerization, deployment strategies\n",
      "\n",
      "---\n",
      "\n",
      "## **6. RAG OPTIMIZATION FROM POC TO PRODUCTION**\n",
      "\n",
      "### **Quality Metrics Framework:**\n",
      "\n",
      "**Retriever Quality**:\n",
      "- **Context precision**: How well ranked are relevant pieces?\n",
      "- **Context recall**: Percentage of ground truth covered?\n",
      "- **Context relevance**: Are retrieved passages relevant (excluding extraneous)?\n",
      "\n",
      "**Generator Quality**:\n",
      "- **Context utilization**: How effectively does generator use source material?\n",
      "- **Noise sensitivity**: Does generator include inaccurate details?\n",
      "- **Hallucination**: Incorrect claims not in source data?\n",
      "- **Faithfulness**: Alignment with source material?\n",
      "\n",
      "**Overall Quality**:\n",
      "- **Precision/Recall/Answer similarity**: Traditional ML metrics\n",
      "\n",
      "### **Advanced RAG Techniques:**\n",
      "\n",
      "**Retrieval Optimization**:\n",
      "- **Advanced parsing**: Use FMs to handle tables and unstructured data\n",
      "- **Hierarchical chunking**: Preserve document relationships\n",
      "- **Semantic chunking**: Maintain contextual relationships\n",
      "- **Auto-generated query filters**: AI-driven metadata filtering\n",
      "- **Query decomposition**: Split complex queries into simpler subqueries\n",
      "\n",
      "**Generation Optimization**:\n",
      "- **Reranking**: Use LLMs to re-order retrieved documents by relevance\n",
      "- **Inference parameter tuning**: Adjust temperature, top-k/p sampling\n",
      "- **Prompt customization**: Clear instructions vs. ambiguous prompts\n",
      "\n",
      "**Cost & Latency Optimization**:\n",
      "- **Semantic caching**: Cache repeated prompt prefixes in Amazon MemoryDB\n",
      "- **Query batching**: Improve throughput and reduce resource usage\n",
      "- **Streaming architecture**: Process results progressively\n",
      "\n",
      "### **Production Architecture Example:**\n",
      "- **Vector database**: Encrypted, tenant-isolated storage\n",
      "- **Application layer**: Frontend + orchestration logic\n",
      "- **Foundational services**: Evaluation, guardrails (shared)\n",
      "- **Central aggregation**: Logs, metrics, traces for analysis\n",
      "\n",
      "---\n",
      "\n",
      "## **7. BROWSER AUTOMATION AGENTS: AMAZON NOVA ACT SDK**\n",
      "\n",
      "For web-based workflow automation, AWS offers **Amazon Nova Act SDK**:\n",
      "\n",
      "### **Advantages Over Traditional Automation:**\n",
      "- **Reliability**: 90%+ accuracy on enterprise workflows vs. brittle rule-based frameworks\n",
      "- **Generalization**: Single script works across diverse interfaces (e.g., multiple hotel payment forms)\n",
      "- **Maintenance**: Adapts to UI changes without selector rewrites\n",
      "\n",
      "### **Production Integration:**\n",
      "- **IAM security**: AWS identity management\n",
      "- **S3 integration**: Data storage and policy control\n",
      "- **Amazon Bedrock AgentCore Browser Tool**: Scalable cloud-based browser execution\n",
      "- **Observability**: Live viewing, CloudTrail logging, session replay\n",
      "- **VM-level isolation**: Enterprise-grade security\n",
      "\n",
      "### **Use Cases in Production:**\n",
      "- **Automated data entry**: CRMs, HR tools, finance platforms\n",
      "- **Form filling**: Complex state-specific applications (benefits, licenses)\n",
      "- **Customer support augmentation**: Multi-tool workflows at scale\n",
      "- **Credential verification**: Compliance-heavy portal navigation\n",
      "- **UX/QA testing**: Natural-language test plans auto-converted to suites\n",
      "\n",
      "---\n",
      "\n",
      "## **8. DEPLOYMENT BEST PRACTICES CHECKLIST**\n",
      "\n",
      "### **Pre-Production (Development Phase)**\n",
      "- ✅ Establish resilience risk analysis across seven dimensions\n",
      "- ✅ Define quality metrics and evaluation framework\n",
      "- ✅ Implement prompt caching for high-latency scenarios\n",
      "- ✅ Set recursion limits and max completion tokens\n",
      "- ✅ Build deterministic orchestration workflows\n",
      "- ✅ Test fault injection and chaos scenarios\n",
      "\n",
      "### **Security & Compliance**\n",
      "- ✅ Enable session isolation with dedicated microVMs\n",
      "- ✅ Implement embedded identity (AgentCore Identity or IAM)\n",
      "- ✅ Apply input/output guardrails (hallucination, compliance)\n",
      "- ✅ Encrypt data in transit (TLS) and at rest (AWS KMS)\n",
      "- ✅ Configure multi-tenant data isolation strategy\n",
      "- ✅ Set up audit logging (CloudTrail) and access controls\n",
      "\n",
      "### **Scalability & Performance**\n",
      "- ✅ Calculate capacity needs using RPM/TPM equations\n",
      "- ✅ Implement intelligent request queuing with priorities\n",
      "- ✅ Configure reserved concurrency for critical tools\n",
      "- ✅ Set up load balancing and multi-Region failover\n",
      "- ✅ Enable streaming for chat applications\n",
      "- ✅ Plan for graceful degradation under overload\n",
      "\n",
      "### **Observability & Operations**\n",
      "- ✅ Collect traditional metrics (CloudWatch) + AI-specific traces\n",
      "- ✅ Implement evaluation pipelines for continuous monitoring\n",
      "- ✅ Set up centralized log aggregation\n",
      "- ✅ Create playbooks for infrastructure and agent-specific issues\n",
      "- ✅ Enable live session viewing and replay for troubleshooting\n",
      "- ✅ Configure real-time alerts for anomalies\n",
      "\n",
      "### **Cost Management**\n",
      "- ✅ Use AgentCore Runtime consumption-based pricing\n",
      "- ✅ Implement prompt c"
     ]
    }
   ],
   "source": [
    "session_id=str(uuid4())\n",
    "payload = json.dumps({\"prompt\": \"search about deploying gen ai agents to production\"}).encode()\n",
    "invoke_agent_core(agent_arn, payload, session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca83bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll conduct a deep dive into evaluation frameworks for generative AI agents and systems. Let me search for comprehensive information on this critical topic.Excellent! I found many relevant resources. Let me extract the key articles on evaluation frameworks:Perfect! Now I have comprehensive information on evaluation frameworks. Let me compile this into a detailed deep dive:\n",
      "\n",
      "---\n",
      "\n",
      "# **DEEP DIVE: Evaluation Frameworks for Generative AI Agents and Systems**\n",
      "\n",
      "## **1. OVERVIEW: WHY EVALUATION FRAMEWORKS ARE CRITICAL**\n",
      "\n",
      "Evaluation frameworks are essential for Gen AI because:\n",
      "\n",
      "- **AI systems are probabilistic**: Unlike traditional software, outputs vary based on model reasoning and stochasticity\n",
      "- **Multi-dimensional quality**: Must assess quality, cost, latency, safety, and responsible AI dimensions simultaneously\n",
      "- **Production readiness**: Organizations need confidence before deploying to users\n",
      "- **Continuous improvement**: Evaluation enables feedback loops to iteratively optimize systems\n",
      "\n",
      "Without proper evaluation frameworks, organizations operate \"blind\" to actual system performance.\n",
      "\n",
      "---\n",
      "\n",
      "## **2. AMAZON BEDROCK EVALUATIONS: THE UNIFIED FRAMEWORK**\n",
      "\n",
      "AWS offers **Amazon Bedrock Evaluations**, a comprehensive, generally available service (as of April 2025) that addresses evaluation across multiple scenarios:\n",
      "\n",
      "### **2.1 Key Capabilities**\n",
      "\n",
      "**LLM-as-a-Judge (LLMaaJ)**\n",
      "- Uses foundation models as automated evaluators\n",
      "- Provides up to 98% cost savings vs. human evaluation\n",
      "- Reduces evaluation time from weeks to hours\n",
      "- Maintains consistent evaluation standards at scale\n",
      "\n",
      "**Bring Your Own Inference (BYOI)**\n",
      "- Evaluate RAG systems or models **anywhere** they're deployed\n",
      "- Not limited to Amazon Bedrock hosted systems\n",
      "- Support for on-premises, multi-cloud, and external deployments\n",
      "- Unified evaluation across heterogeneous environments\n",
      "\n",
      "**Comprehensive Metric Coverage**\n",
      "- Quality metrics (correctness, completeness, faithfulness)\n",
      "- User experience metrics (helpfulness, coherence, relevance)\n",
      "- Instruction following (professional style, following instructions)\n",
      "- Safety metrics (harmfulness, stereotyping, refusal handling)\n",
      "\n",
      "---\n",
      "\n",
      "## **3. TWO DISTINCT EVALUATION TYPES**\n",
      "\n",
      "### **3.1 Model Evaluation (LLM-as-a-Judge)**\n",
      "\n",
      "**Purpose**: Assess foundation model or custom model performance on specific tasks\n",
      "\n",
      "**Architecture**:\n",
      "```\n",
      "Input Dataset (Prompts ± Ground Truth)\n",
      "        ↓\n",
      "Evaluator Model (Judge) \n",
      "        ↓\n",
      "Evaluation Job Processing\n",
      "        ↓\n",
      "Automated Report Generation (Metrics, Scores, Insights)\n",
      "        ↓\n",
      "Analysis & Action\n",
      "```\n",
      "\n",
      "**Built-in Metrics for LLMaaJ**:\n",
      "1. **Builtin.Correctness** - Factual accuracy of responses\n",
      "2. **Builtin.Completeness** - Thoroughness of response\n",
      "3. **Builtin.Faithfulness** - Hallucination detection (response adheres to ground truth)\n",
      "4. **Builtin.Helpfulness** - Practical utility for users\n",
      "5. **Builtin.Coherence** - Logical flow and structure\n",
      "6. **Builtin.Relevance** - Appropriateness to query\n",
      "7. **Builtin.FollowingInstructions** - Compliance with system instructions\n",
      "8. **Builtin.ProfessionalStyleAndTone** - Maintains desired voice/style\n",
      "9. **Builtin.Harmfulness** - Safety metric (responsible AI)\n",
      "10. **Builtin.Stereotyping** - Bias detection (responsible AI)\n",
      "11. **Builtin.Refusal** - Appropriate refusal of unsafe requests (responsible AI)\n",
      "\n",
      "**Dataset Format (JSONL)**:\n",
      "```json\n",
      "{\n",
      "    \"prompt\": \"What is machine learning?\",\n",
      "    \"referenceResponse\": \"Machine learning is a subset of AI...\",\n",
      "    \"category\": \"technical\",\n",
      "    \"modelResponses\": [{\n",
      "        \"response\": \"Model's response text\",\n",
      "        \"modelIdentifier\": \"third-party-model\"\n",
      "    }]\n",
      "}\n",
      "```\n",
      "\n",
      "**Key Features**:\n",
      "- Requires evaluator model (judge) - pre-curated by AWS\n",
      "- Optional ground truth (referenceResponse)\n",
      "- Support for categorization and segmentation\n",
      "- Flexible input - works with Amazon Bedrock models or external models (BYOI)\n",
      "\n",
      "### **3.2 RAG Evaluation**\n",
      "\n",
      "**Purpose**: Assess Retrieval Augmented Generation systems across retrieval and generation dimensions\n",
      "\n",
      "**Three Evaluation Modes**:\n",
      "\n",
      "1. **Retrieve Only** - Evaluate retrieval quality alone\n",
      "2. **Retrieve and Generate** - Evaluate full RAG pipeline\n",
      "3. **Custom Retrieve Strategy** - Evaluate your custom retrieval logic\n",
      "\n",
      "**RAG-Specific Metrics**:\n",
      "\n",
      "#### **Retrieval Quality Metrics**:\n",
      "- **Context Precision**: How well ranked are relevant pieces? (0-1, higher is better)\n",
      "- **Context Recall**: What percentage of ground truth is covered? (0-1, higher is better)\n",
      "- **Context Relevance**: Are retrieved passages relevant? (0-1, higher is better)\n",
      "\n",
      "#### **Generation Quality Metrics**:\n",
      "- **Context Utilization**: How effectively does generator use source material?\n",
      "- **Noise Sensitivity**: Does generator include inaccurate details from context?\n",
      "- **Hallucination**: Incorrect claims not in source data?\n",
      "- **Self-Knowledge**: Accurate statements not in retrieved chunks\n",
      "- **Faithfulness**: Response alignment with source material\n",
      "\n",
      "#### **Citation Quality Metrics** (NEW):\n",
      "- **Citation Precision** (0-1): Are cited passages actually used in response?\n",
      "  - Prevents over-citing irrelevant sources\n",
      "  - Perfect score (1) = all citations were relevant and used\n",
      "  \n",
      "- **Citation Coverage** (0-1): Is all citable information properly cited?\n",
      "  - Measures if information derived from passages is properly credited\n",
      "  - Compares faithfulness to cited vs. full retrieved passages\n",
      "  - Perfect score (1) = all information that could be cited IS cited\n",
      "\n",
      "**Dataset Format (JSONL) - Retrieve and Generate**:\n",
      "```json\n",
      "{\n",
      "    \"conversationTurns\": [{\n",
      "        \"prompt\": {\n",
      "            \"content\": [{\"text\": \"What is Amazon's SEC file number?\"}]\n",
      "        },\n",
      "        \"referenceResponses\": [{\n",
      "            \"content\": [{\"text\": \"Amazon's SEC file number is 000-22513.\"}]\n",
      "        }],\n",
      "        \"output\": {\n",
      "            \"text\": \"Amazon's SEC file number is 000-22513.\",\n",
      "            \"modelIdentifier\": \"third-party-RAG\",\n",
      "            \"knowledgeBaseIdentifier\": \"third-party-RAG\",\n",
      "            \"retrievedPassages\": {\n",
      "                \"retrievalResults\": [\n",
      "                    {\"content\": {\"text\": \"Commission File No. 000-22513\"}}\n",
      "                ]\n",
      "            },\n",
      "            \"citations\": [\n",
      "                {\n",
      "                    \"generatedResponsePart\": {\n",
      "                        \"textResponsePart\": {\n",
      "                            \"span\": {\"start\": 0, \"end\": 22},\n",
      "                            \"text\": \"Amazon's SEC file number\"\n",
      "                        }\n",
      "                    },\n",
      "                    \"retrievedReferences\": [\n",
      "                        {\"content\": {\"text\": \"Commission File No. 000-22513\"}}\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    }]\n",
      "}\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## **4. SYNTHETIC DATA GENERATION FOR EVALUATION**\n",
      "\n",
      "### **4.1 The Challenge**\n",
      "\n",
      "Evaluating RAG systems requires:\n",
      "- High-quality question-answer pairs\n",
      "- Corresponding relevant context\n",
      "- Real-world representativeness\n",
      "- At scale (hundreds/thousands of samples)\n",
      "\n",
      "**Problem**: Real user data is scarce, expensive to collect, and often not available early in development\n",
      "\n",
      "**Solution**: Generate synthetic evaluation datasets using LLMs\n",
      "\n",
      "### **4.2 Synthetic Generation Workflow**\n",
      "\n",
      "**Step 1: Load and Prepare Data**\n",
      "```python\n",
      "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "loader = PyPDFDirectoryLoader(\"./documents/\")  \n",
      "documents = loader.load()\n",
      "\n",
      "text_splitter = RecursiveCharacterTextSplitter(\n",
      "    chunk_size=1500,  \n",
      "    chunk_overlap=100,\n",
      "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
      ")\n",
      "docs = text_splitter.split_documents(documents)\n",
      "```\n",
      "\n",
      "**Step 2: Generate Initial Questions**\n",
      "- Use LLM to analyze document chunks\n",
      "- Generate questions users would ask\n",
      "- Rules ensure relevance, clarity, and answerability\n",
      "\n",
      "Example prompt template:\n",
      "```\n",
      "Here is context: {context}\n",
      "\n",
      "Generate 1 question that can be answered using this context:\n",
      "1. Question should make sense without given context\n",
      "2. Question should be fully answered by context\n",
      "3. Moderate difficulty\n",
      "4. No more than 10 words\n",
      "5. Use abbreviations where possible\n",
      "\n",
      "Output only the question with \"?\" at end.\n",
      "```\n",
      "\n",
      "**Result**: \"What was the YoY growth of AWS revenue in 2021?\"\n",
      "\n",
      "**Step 3: Generate Answers**\n",
      "- Use same LLM to generate reference answers\n",
      "- Based on question + context\n",
      "- Verify answer can be extracted from context\n",
      "\n",
      "```\n",
      "Generate an answer to the question: {question}\n",
      "Only based on context: {context}\n",
      "Output only the answer as one sentence.\n",
      "```\n",
      "\n",
      "**Result**: \"The AWS revenue grew 37% year-over-year in 2021.\"\n",
      "\n",
      "**Step 4: Extract Relevant Source**\n",
      "- Extract sentences from context that answer the question\n",
      "- Creates verifiable reference\n",
      "- Enables ground truth validation\n",
      "\n",
      "```\n",
      "Extract relevant sentences from context that answer: {question}\n",
      "Output only sentences from context, one per line.\n",
      "```\n",
      "\n",
      "**Result**: \"This shift by so many companies helped re-accelerate AWS's revenue growth to 37% Y oY in 2021.\"\n",
      "\n",
      "**Step 5: Evolve/Refine Questions**\n",
      "- Rewrite in alternative styles\n",
      "- Use indirect phrasing\n",
      "- Add abbreviations\n",
      "- Increases dataset diversity\n",
      "\n",
      "```\n",
      "Rewrite this question more indirectly and compressed:\n",
      "{question}\n",
      "\n",
      "Rules:\n",
      "1. Make more indirect\n",
      "2. Make shorter\n",
      "3. Use abbreviations\n",
      "```\n",
      "\n",
      "**Result**: \"AWS rev YoY growth in '21?\"\n",
      "\n",
      "### **4.3 Cost-Effectiveness**\n",
      "\n",
      "Using Claude 3 Haiku:\n",
      "- **Processing time**: 2.6 seconds per question set (initial, answer, evolved, source)\n",
      "- **Cost**: ~$2.80 USD for 1,000 question-answer pairs\n",
      "- **Scale benefit**: 10,000-100,000 evaluation samples become economical\n",
      "\n",
      "### **4.4 Quality Assurance with Critique Agents**\n",
      "\n",
      "**Two-Stage Evaluation**:\n",
      "\n",
      "1. **Groundedness Check** (1-5 scale):\n",
      "   - 5 = Context provides all info to fully answer question\n",
      "   - 4 = Context provides substantial information\n",
      "   - 3 = Context provides some relevant information\n",
      "   - 2 = Context provides very little\n",
      "   - 1 = Cannot answer from context\n",
      "\n",
      "2. **Relevance Check** (1-5 scale):\n",
      "   - Practical value for target users?\n",
      "   - Domain-specific applicability?\n",
      "   - Clarity and well-defined?\n",
      "   - Requires substantive answer?\n",
      "   - Real-world applicability?\n",
      "\n",
      "Example evaluation result:\n",
      "```\n",
      "Groundedness: 5 - \"Context provides exact information needed\"\n",
      "Relevance: 5 - \"Highly relevant for financial analysts, AWS is key segment\"\n",
      "```\n",
      "\n",
      "### **4.5 Best Practices for Synthetic Data**\n",
      "\n",
      "1. **Combine with real-world data** - Synthetic alone may miss edge cases\n",
      "2. **Choose different model for generation** - Avoid self-enhancement bias\n",
      "3. **Implement robust QA** - Use critique agents, human review, automated checks\n",
      "4. **Iterate and refine** - Treat as continuous improvement process\n",
      "5. **Domain-specific customization** - Fine-tune LLM with domain knowledge for specialized domains\n",
      "\n",
      "---\n",
      "\n",
      "## **5. LLM-AS-A-JUDGE FRAMEWORK: DEEP DIVE**\n",
      "\n",
      "### **5.1 How LLMaaJ Works**\n",
      "\n",
      "**Traditional Human Evaluation Problems**:\n",
      "- Cost: $1-5 per judgment\n",
      "- Time: Weeks to evaluate large datasets\n",
      "- Inconsistency: Different evaluators rate differently\n",
      "- Scalability: Limits number of evaluations\n",
      "\n",
      "**LLMaaJ Solution**:\n",
      "```\n",
      "Model Response → Judge LLM → Evaluation Score + Reasoning\n",
      "                (Claude, Haiku)\n",
      "                \n",
      "Per-judgment cost: ~$0.01-0.05\n",
      "Evaluation time: Hours for large datasets\n",
      "Consistency: Same judge for all samples\n",
      "Scalability: Unlimited dataset size\n",
      "```\n",
      "\n",
      "### **5.2 Key Features of LLMaaJ**\n",
      "\n",
      "**1. Automated Intelligent Evaluation**\n",
      "- Up to 98% cost savings vs. human evaluation\n",
      "- Human-like quality\n",
      "- Consistent standards across datasets\n",
      "\n",
      "**2. Comprehensive Metric Categories**\n",
      "- Quality assessment (correctness, completeness, faithfulness)\n",
      "- User experience (helpfulness, coherence, relevance)\n",
      "- Instruction compliance (following instructions, professional style)\n",
      "- Safety monitoring (harmfulness, stereotyping, refusal)\n",
      "\n",
      "**3. Seamless Integration**\n",
      "- Works with Amazon Bedrock models\n",
      "- Works with BYOI responses (external models)\n",
      "- Compatible with existing evaluation features\n",
      "\n",
      "**4. Curated Judge Models**\n",
      "- AWS pre-selects high-quality evaluator models\n",
      "- Optimized prompts built in\n",
      "- No need to bring external judges\n",
      "\n",
      "**5. Cost-Effective Scaling**\n",
      "- Perform comprehensive evaluations without traditional costs\n",
      "- Automated process maintains high quality\n",
      "\n",
      "### **5.3 Model Comparison Using LLMaaJ**\n",
      "\n",
      "**Use Case**: Determine which model is best for your use case\n",
      "\n",
      "```python\n",
      "GENERATOR_MODELS = [\n",
      "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
      "    \"amazon.nova-micro-v1:0\"\n",
      "]\n",
      "EVALUATOR_MODEL = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
      "\n",
      "# Run multiple jobs with same evaluator\n",
      "for generator_model in GENERATOR_MODELS:\n",
      "    create_llm_judge_evaluation(\n",
      "        generator_model_id=generator_model,\n",
      "        evaluator_model_id=EVALUATOR_MODEL,  # Same across all\n",
      "        ...\n",
      "    )\n",
      "```\n",
      "\n",
      "**Key Insight**: Use **same evaluator model** across all comparisons for standardized benchmarking\n",
      "\n",
      "### **5.4 Correlation Analysis**\n",
      "\n",
      "**Spearman's Rank Correlation** measures how similarly different models respond:\n",
      "\n",
      "```python\n",
      "from scipy import stats\n",
      "\n",
      "correlation, p_value = stats.spearmanr(scores1, scores2)\n",
      "```\n",
      "\n",
      "- **Correlation ~1.0**: Models agree strongly\n",
      "- **Correlation ~0**: Models diverge significantly\n",
      "- **Correlation <0**: Models inverse relationship\n",
      "\n",
      "**Actionable Insight**: \n",
      "- Correlation 0.9+ = Models are interchangeable\n",
      "- Correlation 0.5-0.8 = Models handle some cases differently\n",
      "- Correlation <0.5 = Choose based on specific strengths\n",
      "\n",
      "---\n",
      "\n",
      "## **6. CUSTOM METRICS: TAILORED EVALUATION**\n",
      "\n",
      "### **6.1 When Built-in Metrics Aren't Enough**\n",
      "\n",
      "**Use Cases for Custom Metrics**:\n",
      "- Brand voice compliance\n",
      "- Domain-specific quality criteria\n",
      "- Custom categorical classifications\n",
      "- Industry-specific compliance\n",
      "- Application-specific rubrics\n",
      "\n",
      "**Example**: Retail chatbot needs \"helpfulness\" defined as:\n",
      "- Product knowledge accuracy\n",
      "- Sales-appropriate tone\n",
      "- Urgency-sensitive responses\n",
      "- Competitor mention handling\n",
      "\n",
      "### **6.2 Custom Metric Definition Structure**\n",
      "\n",
      "**JSON Schema**:\n",
      "```json\n",
      "{\n",
      "    \"customMetricDefinition\": {\n",
      "        \"metricName\": \"comprehensiveness\",\n",
      "        \"instructions\": \"Your prompt with {{variable}} placeholders\",\n",
      "        \"ratingScale\": [\n",
      "            {\n",
      "                \"definition\": \"Very comprehensive\",\n",
      "                \"value\": {\"floatValue\": 10}\n",
      "            },\n",
      "            {\n",
      "                \"definition\": \"Not comprehensive\",\n",
      "                \"value\": {\"floatValue\": 1}\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "**Three Scoring Types**:\n",
      "1. **Numerical Scale**: floatValue (1-10, 0.0-1.0, etc.)\n",
      "2. **Categorical Scale**: stringValue (\"Good\", \"Bad\", \"Neutral\")\n",
      "3. **No Scale**: Open-ended text responses\n",
      "\n",
      "### **6.3 Template Variables for Data Injection**\n",
      "\n",
      "**Available Variables**:\n",
      "\n",
      "| Variable | Pulls From | Use Case |\n",
      "|----------|-----------|----------|\n",
      "| `{{prompt}}` | Input query | Reference what user asked |\n",
      "| `{{prediction}}` | Model response | Judge the generated output |\n",
      "| `{{ground_truth}}` | Reference response | Compare against expected answer |\n",
      "| `{{context}}` | Retrieved passages (RAG) | Evaluate relevance to source material |\n",
      "| `{{reference_contexts}}` | Expected retrieved passages | Validate retrieval quality |\n",
      "\n",
      "### **6.4 Example: Brand Voice Custom Metric**\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"customMetricDefinition\": {\n",
      "        \"metricName\": \"brand_voice_adherence\",\n",
      "        \"instructions\": \"\"\"\n",
      "Your role is to evaluate how well the response adheres to our brand voice.\n",
      "Our brand voice is: conversational, friendly, empowering, and solution-focused.\n",
      "\n",
      "Evaluate the response: {{prediction}}\n",
      "\n",
      "For query: {{prompt}}\n",
      "\n",
      "Rate how well the response embodies our brand voice:\n",
      "- Conversational"
     ]
    }
   ],
   "source": [
    "payload = json.dumps({\"prompt\": \"can you dive deep in evaluation frameworks?\"}).encode()\n",
    "invoke_agent_core(agent_arn, payload, session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765cc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986b7e3f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
